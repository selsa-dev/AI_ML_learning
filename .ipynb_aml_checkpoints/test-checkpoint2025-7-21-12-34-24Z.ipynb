{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow Version: \" + tf.__version__)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "TensorFlow Version: 2.19.0\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1755100436549
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://book24.ru'  # Replace with the URL of the website you want to scrape\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    print('Request successful!')\n",
        "else:\n",
        "    print('Failed to retrieve the webpage')\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Print the title of the webpage to verify\n",
        "print(soup.title.text)\n",
        "\n",
        "# Find the table containing the data\n",
        "table = soup.find('main', {'id': 'main'})  # Replace 'data-table' with the actual id or class of the table\n",
        "\n",
        "# Extract table rows\n",
        "rows = table.find_all('a')\n",
        "\n",
        "# Loop through the rows and extract data\n",
        "data = []\n",
        "for row in rows:\n",
        "    cols = [row.text.strip()]\n",
        "    data.append(cols)\n",
        "\n",
        "# Convert the data into a pandas DataFrame for easier manipulation\n",
        "df = pd.DataFrame(data)  # Replace with actual column names\n",
        "print(df.head(10))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Request successful!\nBook24.ru – книжный интернет магазин – купить книги, читать отзывы. Доставка по Москве, СПб и в любой город - Главная страница\n                           0\n0         Книги с автографом\n1  Художественная литература\n2         Боевики и триллеры\n3            Военные боевики\n4         Зарубежные боевики\n5       Криминальные боевики\n6            Русские боевики\n7                   Триллеры\n8                  Детективы\n9       Зарубежные детективы\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1755250208045
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Send an HTTP request to the webpage\n",
        "url = 'https://en.wikipedia.org/wiki/Cloud-computing_comparison'  \n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Print the title of the webpage to verify\n",
        "print(\"Title: \" + soup.title.text)\n",
        "\n",
        "# Find the table containing the data (selecting the first table by default)\n",
        "table = soup.find('table')\n",
        "\n",
        "# Extract table rows\n",
        "rows = table.find_all('tr')\n",
        "\n",
        "# Extract headers from the first row (using <th> tags)\n",
        "headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "\n",
        "# Loop through the rows and extract data (skip the first row with headers)\n",
        "data = []\n",
        "for row in rows[1:]:  # Start from the second row onwards\n",
        "    cols = row.find_all('td')\n",
        "    cols = [col.text.strip() for col in cols]\n",
        "    data.append(cols)\n",
        "\n",
        "# Convert the data into a pandas DataFrame, using the extracted headers as column names\n",
        "df = pd.DataFrame(data, columns=headers)\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify\n",
        "print(df.head())  \n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('scraped_data.csv', index=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Title: Cloud-computing comparison - Wikipedia\n                      Provider Launched Block storage Assignable IPs  \\\n0        Google Cloud Platform     2013           Yes             No   \n1  Oracle Cloud Infrastructure     2014           Yes            Yes   \n2          Amazon Web Services     2006           Yes            Yes   \n3                    IBM Cloud     2005           Yes            Yes   \n4              Microsoft Azure     2010           Yes            Yes   \n\n  SMTP support IOPS Guaranteed minimum Security  \\\n0        No[1]                     Yes   Yes[2]   \n1          Yes                     Yes   Yes[5]   \n2   Partial[6]                     Yes   Yes[7]   \n3        No[9]                     Yes  Yes[10]   \n4      Yes[11]                     Yes  Yes[12]   \n\n                                           Locations             Notes  \n0  br, ca, cl, us, be, ch, de, es, fi, it, po, nl...  SMTP blocked.[4]  \n1  us, ca, br, de, uk, nl, ch, in, aus, jp, kr, saud                    \n2  us, ca, br, ie, de, uk, cn, sg, au, jp, kr, in...   List of bugs[8]  \n3  us, gb, fr, de, nl, in, au, hk, kr, it, jp, no...                    \n4  ca, us, br, ie, nl, de, uk, cn, au, jp, in, kr...  List of bugs[13]  \n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1755250270176
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import missingno as msno  # Optional: for visualizing missing data\n",
        "\n",
        "# Load your dataset into a pandas DataFrame\n",
        "df = pd.read_csv('ncr_ride_bookings.csv')  # Replace 'your_dataset.csv' with your actual file path\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "# print(df.head())\n",
        "\n",
        "# Visualize missing data (optional)\n",
        "# msno.matrix(df)\n",
        "# msno.heatmap(df)\n",
        "\n",
        "# Drop rows with missing values\n",
        "# df_cleaned = df.dropna(axis=1)\n",
        "# msno.matrix(df_cleaned)\n",
        "\n",
        "# Or, fill missing values with the mean\n",
        "# df_filled = df.fillna(df.mean())\n",
        "# msno.matrix(df_filled)\n",
        "\n",
        "# Using Z-score to identify outliers\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "# df_cleaned = df.dropna()\n",
        "# z_scores = stats.zscore(df_cleaned.select_dtypes(include=[np.number]))\n",
        "# print(z_scores)\n",
        "# z_scores = np.abs(stats.zscore(df['Avg VTAT']))\n",
        "# print(z_scores)\n",
        "# outliers = df[z_scores > 3]\n",
        "# print(f\"outliers: {outliers}\")\n",
        "\n",
        "# Using IQR to identify outliers\n",
        "# Q1 = df['Avg VTAT'].quantile(0.25)\n",
        "# Q3 = df['Avg VTAT'].quantile(0.75)\n",
        "# IQR = Q3 - Q1\n",
        "# outliers = df[(df['Avg VTAT'] < (Q1 - 1.5 * IQR)) | (df['Avg VTAT'] > (Q3 + 1.5 * IQR))]\n",
        "# print(f\"outliers 2: {outliers}\")\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df['scaled_column'] = scaler.fit_transform(df[['Avg VTAT']])\n",
        "print(df.head())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "         Date      Time    Booking ID   Booking Status   Customer ID  \\\n0  2024-03-23  12:29:38  \"CNR5884300\"  No Driver Found  \"CID1982111\"   \n1  2024-11-29  18:01:39  \"CNR1326809\"       Incomplete  \"CID4604802\"   \n2  2024-08-23  08:56:10  \"CNR8494506\"        Completed  \"CID9202816\"   \n3  2024-10-21  17:17:25  \"CNR8906825\"        Completed  \"CID2610914\"   \n4  2024-09-16  22:08:00  \"CNR1950162\"        Completed  \"CID9933542\"   \n\n    Vehicle Type      Pickup Location      Drop Location  Avg VTAT  Avg CTAT  \\\n0          eBike          Palam Vihar            Jhilmil       NaN       NaN   \n1       Go Sedan        Shastri Nagar  Gurgaon Sector 56       4.9      14.0   \n2           Auto              Khandsa      Malviya Nagar      13.4      25.8   \n3  Premier Sedan  Central Secretariat           Inderlok      13.1      28.5   \n4           Bike     Ghitorni Village        Khan Market       5.3      19.6   \n\n   ...  Cancelled Rides by Driver Driver Cancellation Reason  \\\n0  ...                        NaN                        NaN   \n1  ...                        NaN                        NaN   \n2  ...                        NaN                        NaN   \n3  ...                        NaN                        NaN   \n4  ...                        NaN                        NaN   \n\n   Incomplete Rides Incomplete Rides Reason  Booking Value Ride Distance  \\\n0               NaN                     NaN            NaN           NaN   \n1               1.0       Vehicle Breakdown          237.0          5.73   \n2               NaN                     NaN          627.0         13.58   \n3               NaN                     NaN          416.0         34.02   \n4               NaN                     NaN          737.0         48.21   \n\n   Driver Ratings  Customer Rating  Payment Method  scaled_column  \n0             NaN              NaN             NaN            NaN  \n1             NaN              NaN             UPI       0.161111  \n2             4.9              4.9      Debit Card       0.633333  \n3             4.6              5.0             UPI       0.616667  \n4             4.1              4.3             UPI       0.183333  \n\n[5 rows x 22 columns]\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1755519549907
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def load_data(filepath):\n",
        "    return pd.read_csv(filepath)\n",
        "\n",
        "def handle_missing_values(df):\n",
        "    return df.fillna(df.mean())\n",
        "\n",
        "def remove_outliers(df):\n",
        "    z_scores = np.abs(stats.zscore(df))\n",
        "    return df[(z_scores < 3).all(axis=1)]\n",
        "\n",
        "def scale_data(df):\n",
        "    scaler = StandardScaler()\n",
        "    return pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "def encode_categorical(df, categorical_columns):\n",
        "    return pd.get_dummies(df, columns=categorical_columns)\n",
        "\n",
        "def save_data(df, output_filepath):\n",
        "    df.to_csv(output_filepath, index=False)\n",
        "\n",
        "# Example usage:\n",
        "df = load_data('academicStress.csv')\n",
        "df = handle_missing_values(df)\n",
        "# df = remove_outliers(df)\n",
        "df = scale_data(df)\n",
        "df = encode_categorical(df, ['Peer pressure'])\n",
        "save_data(df, 'cleaned_preprocessed_data.csv')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_3039/3526320593.py:9: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n  return df.fillna(df.mean())\n"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: '24/07/2025 22:05:39'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_3039/3526320593.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Example usage:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'academicStress.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_missing_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# df = remove_outliers(df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Peer pressure'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0msave_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cleaned_preprocessed_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3039/3526320593.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscale_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             return_tuple = (\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 )\n\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \"\"\"\n\u001b[1;32m    876\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1469\u001b[0m                 skip_parameter_validation=(\n\u001b[1;32m   1470\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m         \"\"\"\n\u001b[1;32m    913\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1009\u001b[0m                         )\n\u001b[1;32m   1010\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m                 raise ValueError(\n\u001b[1;32m   1015\u001b[0m                     \u001b[0;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m                 ) from complex_warning\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2070\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '24/07/2025 22:05:39'"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1755521088109
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import stats\n",
        "\n",
        "# Create a dummy dataset\n",
        "np.random.seed(0)\n",
        "dummy_data = {\n",
        "    'Feature1': np.random.normal(100, 10, 100).tolist() + [np.nan, 200],  # Normally distributed with an outlier\n",
        "    'Feature2': np.random.randint(0, 100, 102).tolist(),  # Random integers\n",
        "    'Category': ['A', 'B', 'C', 'D'] * 25 + [np.nan, 'A'],  # Categorical with some missing values\n",
        "    'Target': np.random.choice([0, 1], 102).tolist()  # Binary target variable\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_dummy = pd.DataFrame(dummy_data)\n",
        "\n",
        "# Display first few rows\n",
        "print(\"Original DataFrame:\")\n",
        "print(df_dummy.head())\n",
        "\n",
        "# Function to load data\n",
        "def load_data(df):\n",
        "    return df.copy()  # Return a copy to avoid modifying the input\n",
        "\n",
        "# Function to handle missing values\n",
        "def handle_missing_values(df):\n",
        "    # Create a copy to avoid modifying the input\n",
        "    df = df.copy()\n",
        "    # Fill numeric columns with mean\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    df.loc[:, numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean(numeric_only=True))\n",
        "    # Fill categorical columns with mode\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    df.loc[:, categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])\n",
        "    return df\n",
        "\n",
        "# Function to remove outliers\n",
        "def remove_outliers(df):\n",
        "    df = df.copy()  # Create a copy\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    z_scores = np.abs(stats.zscore(df[numeric_cols]))\n",
        "    return df[(z_scores < 3).all(axis=1)]  # Remove rows with outliers\n",
        "\n",
        "# Function to scale numeric data using MinMaxScaler\n",
        "def scale_data(df, scaler=None):\n",
        "    df = df.copy()  # Create a copy\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if scaler is None:\n",
        "        scaler = MinMaxScaler()\n",
        "        # Fit and transform on numeric columns\n",
        "        df.loc[:, [f'scaled_{col}' for col in numeric_cols]] = scaler.fit_transform(df[numeric_cols])\n",
        "    else:\n",
        "        # Transform only (for test data)\n",
        "        df.loc[:, [f'scaled_{col}' for col in numeric_cols]] = scaler.transform(df[numeric_cols])\n",
        "    return df, scaler\n",
        "\n",
        "# Function to encode categorical variables\n",
        "def encode_categorical(df, categorical_columns):\n",
        "    return pd.get_dummies(df, columns=categorical_columns)\n",
        "\n",
        "# Function to save data\n",
        "def save_data(df, output_filepath):\n",
        "    df.to_csv(output_filepath, index=False)\n",
        "\n",
        "# Preprocessing pipeline with train-test split\n",
        "df_preprocessed = load_data(df_dummy)\n",
        "df_preprocessed = handle_missing_values(df_preprocessed)\n",
        "df_preprocessed = remove_outliers(df_preprocessed)\n",
        "\n",
        "# Split data into training and test sets\n",
        "train_df, test_df = train_test_split(df_preprocessed, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale training data and get the scaler\n",
        "train_df, scaler = scale_data(train_df)\n",
        "\n",
        "# Scale test data using the same scaler\n",
        "test_df, _ = scale_data(test_df, scaler=scaler)\n",
        "\n",
        "# Encode categorical variables\n",
        "train_df = encode_categorical(train_df, ['Category'])\n",
        "test_df = encode_categorical(test_df, ['Category'])\n",
        "\n",
        "# Display preprocessed training data\n",
        "print(\"\\nPreprocessed Training DataFrame:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# Save preprocessed data\n",
        "save_data(train_df, 'cleaned_preprocessed_train.csv')\n",
        "save_data(test_df, 'cleaned_preprocessed_test.csv')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Original DataFrame:\n     Feature1  Feature2 Category  Target\n0  117.640523        32        A       1\n1  104.001572        70        B       1\n2  109.787380        85        C       0\n3  122.408932        31        D       1\n4  118.675580        13        A       0\n\nPreprocessed Training DataFrame:\n      Feature1  Feature2  Target  scaled_Feature1  scaled_Feature2  \\\n91  112.224451        91       0         0.782840         0.938144   \n64  101.774261        10       0         0.566154         0.103093   \n28  115.327792        61       1         0.847188         0.628866   \n83   84.637563        53       0         0.210823         0.546392   \n5    90.227221        71       1         0.326725         0.731959   \n\n    scaled_Target  Category_A  Category_B  Category_C  Category_D  \n91            0.0           0           0           0           1  \n64            0.0           1           0           0           0  \n28            1.0           1           0           0           0  \n83            0.0           0           0           0           1  \n5             1.0           0           1           0           0  \n"
        }
      ],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1755522232405
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.10 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}