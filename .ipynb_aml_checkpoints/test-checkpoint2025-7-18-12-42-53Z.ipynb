{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow Version: \" + tf.__version__)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "TensorFlow Version: 2.19.0\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1755100436549
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://book24.ru'  # Replace with the URL of the website you want to scrape\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    print('Request successful!')\n",
        "else:\n",
        "    print('Failed to retrieve the webpage')\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Print the title of the webpage to verify\n",
        "print(soup.title.text)\n",
        "\n",
        "# Find the table containing the data\n",
        "table = soup.find('main', {'id': 'main'})  # Replace 'data-table' with the actual id or class of the table\n",
        "\n",
        "# Extract table rows\n",
        "rows = table.find_all('a')\n",
        "\n",
        "# Loop through the rows and extract data\n",
        "data = []\n",
        "for row in rows:\n",
        "    cols = [row.text.strip()]\n",
        "    data.append(cols)\n",
        "\n",
        "# Convert the data into a pandas DataFrame for easier manipulation\n",
        "df = pd.DataFrame(data)  # Replace with actual column names\n",
        "print(df.head(10))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Request successful!\nBook24.ru – книжный интернет магазин – купить книги, читать отзывы. Доставка по Москве, СПб и в любой город - Главная страница\n                           0\n0         Книги с автографом\n1  Художественная литература\n2         Боевики и триллеры\n3            Военные боевики\n4         Зарубежные боевики\n5       Криминальные боевики\n6            Русские боевики\n7                   Триллеры\n8                  Детективы\n9       Зарубежные детективы\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1755250208045
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Send an HTTP request to the webpage\n",
        "url = 'https://en.wikipedia.org/wiki/Cloud-computing_comparison'  \n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Print the title of the webpage to verify\n",
        "print(\"Title: \" + soup.title.text)\n",
        "\n",
        "# Find the table containing the data (selecting the first table by default)\n",
        "table = soup.find('table')\n",
        "\n",
        "# Extract table rows\n",
        "rows = table.find_all('tr')\n",
        "\n",
        "# Extract headers from the first row (using <th> tags)\n",
        "headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "\n",
        "# Loop through the rows and extract data (skip the first row with headers)\n",
        "data = []\n",
        "for row in rows[1:]:  # Start from the second row onwards\n",
        "    cols = row.find_all('td')\n",
        "    cols = [col.text.strip() for col in cols]\n",
        "    data.append(cols)\n",
        "\n",
        "# Convert the data into a pandas DataFrame, using the extracted headers as column names\n",
        "df = pd.DataFrame(data, columns=headers)\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify\n",
        "print(df.head())  \n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('scraped_data.csv', index=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Title: Cloud-computing comparison - Wikipedia\n                      Provider Launched Block storage Assignable IPs  \\\n0        Google Cloud Platform     2013           Yes             No   \n1  Oracle Cloud Infrastructure     2014           Yes            Yes   \n2          Amazon Web Services     2006           Yes            Yes   \n3                    IBM Cloud     2005           Yes            Yes   \n4              Microsoft Azure     2010           Yes            Yes   \n\n  SMTP support IOPS Guaranteed minimum Security  \\\n0        No[1]                     Yes   Yes[2]   \n1          Yes                     Yes   Yes[5]   \n2   Partial[6]                     Yes   Yes[7]   \n3        No[9]                     Yes  Yes[10]   \n4      Yes[11]                     Yes  Yes[12]   \n\n                                           Locations             Notes  \n0  br, ca, cl, us, be, ch, de, es, fi, it, po, nl...  SMTP blocked.[4]  \n1  us, ca, br, de, uk, nl, ch, in, aus, jp, kr, saud                    \n2  us, ca, br, ie, de, uk, cn, sg, au, jp, kr, in...   List of bugs[8]  \n3  us, gb, fr, de, nl, in, au, hk, kr, it, jp, no...                    \n4  ca, us, br, ie, nl, de, uk, cn, au, jp, in, kr...  List of bugs[13]  \n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1755250270176
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import missingno as msno  # Optional: for visualizing missing data\n",
        "\n",
        "# Load your dataset into a pandas DataFrame\n",
        "df = pd.read_csv('ncr_ride_bookings.csv')  # Replace 'your_dataset.csv' with your actual file path\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "# print(df.head())\n",
        "\n",
        "# Visualize missing data (optional)\n",
        "# msno.matrix(df)\n",
        "# msno.heatmap(df)\n",
        "\n",
        "# Drop rows with missing values\n",
        "# df_cleaned = df.dropna(axis=1)\n",
        "# msno.matrix(df_cleaned)\n",
        "\n",
        "# Or, fill missing values with the mean\n",
        "# df_filled = df.fillna(df.mean())\n",
        "# msno.matrix(df_filled)\n",
        "\n",
        "# Using Z-score to identify outliers\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "# df_cleaned = df.dropna()\n",
        "# z_scores = stats.zscore(df_cleaned.select_dtypes(include=[np.number]))\n",
        "# print(z_scores)\n",
        "# z_scores = np.abs(stats.zscore(df['Avg VTAT']))\n",
        "# print(z_scores)\n",
        "# outliers = df[z_scores > 3]\n",
        "# print(f\"outliers: {outliers}\")\n",
        "\n",
        "# Using IQR to identify outliers\n",
        "# Q1 = df['Avg VTAT'].quantile(0.25)\n",
        "# Q3 = df['Avg VTAT'].quantile(0.75)\n",
        "# IQR = Q3 - Q1\n",
        "# outliers = df[(df['Avg VTAT'] < (Q1 - 1.5 * IQR)) | (df['Avg VTAT'] > (Q3 + 1.5 * IQR))]\n",
        "# print(f\"outliers 2: {outliers}\")\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df['scaled_column'] = scaler.fit_transform(df[['Avg VTAT']])\n",
        "print(df.head())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "         Date      Time    Booking ID   Booking Status   Customer ID  \\\n0  2024-03-23  12:29:38  \"CNR5884300\"  No Driver Found  \"CID1982111\"   \n1  2024-11-29  18:01:39  \"CNR1326809\"       Incomplete  \"CID4604802\"   \n2  2024-08-23  08:56:10  \"CNR8494506\"        Completed  \"CID9202816\"   \n3  2024-10-21  17:17:25  \"CNR8906825\"        Completed  \"CID2610914\"   \n4  2024-09-16  22:08:00  \"CNR1950162\"        Completed  \"CID9933542\"   \n\n    Vehicle Type      Pickup Location      Drop Location  Avg VTAT  Avg CTAT  \\\n0          eBike          Palam Vihar            Jhilmil       NaN       NaN   \n1       Go Sedan        Shastri Nagar  Gurgaon Sector 56       4.9      14.0   \n2           Auto              Khandsa      Malviya Nagar      13.4      25.8   \n3  Premier Sedan  Central Secretariat           Inderlok      13.1      28.5   \n4           Bike     Ghitorni Village        Khan Market       5.3      19.6   \n\n   ...  Cancelled Rides by Driver Driver Cancellation Reason  \\\n0  ...                        NaN                        NaN   \n1  ...                        NaN                        NaN   \n2  ...                        NaN                        NaN   \n3  ...                        NaN                        NaN   \n4  ...                        NaN                        NaN   \n\n   Incomplete Rides Incomplete Rides Reason  Booking Value Ride Distance  \\\n0               NaN                     NaN            NaN           NaN   \n1               1.0       Vehicle Breakdown          237.0          5.73   \n2               NaN                     NaN          627.0         13.58   \n3               NaN                     NaN          416.0         34.02   \n4               NaN                     NaN          737.0         48.21   \n\n   Driver Ratings  Customer Rating  Payment Method  scaled_column  \n0             NaN              NaN             NaN            NaN  \n1             NaN              NaN             UPI       0.161111  \n2             4.9              4.9      Debit Card       0.633333  \n3             4.6              5.0             UPI       0.616667  \n4             4.1              4.3             UPI       0.183333  \n\n[5 rows x 22 columns]\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1755519549907
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def load_data(filepath):\n",
        "    return pd.read_csv(filepath)\n",
        "\n",
        "def handle_missing_values(df):\n",
        "    return df.fillna(df.mean())\n",
        "\n",
        "def remove_outliers(df):\n",
        "    z_scores = np.abs(stats.zscore(df))\n",
        "    return df[(z_scores < 3).all(axis=1)]\n",
        "\n",
        "def scale_data(df):\n",
        "    scaler = StandardScaler()\n",
        "    return pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "def encode_categorical(df, categorical_columns):\n",
        "    return pd.get_dummies(df, columns=categorical_columns)\n",
        "\n",
        "def save_data(df, output_filepath):\n",
        "    df.to_csv(output_filepath, index=False)\n",
        "\n",
        "# Example usage:\n",
        "df = load_data('academicStress.csv')\n",
        "df = handle_missing_values(df)\n",
        "df = remove_outliers(df)\n",
        "df = scale_data(df)\n",
        "df = encode_categorical(df, ['categorical_column_name'])\n",
        "save_data(df, 'cleaned_preprocessed_data.csv')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_3039/1518731213.py:9: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n  return df.fillna(df.mean())\n"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for /: 'str' and 'int'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m df \u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macademicStress.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m df \u001b[38;5;241m=\u001b[39m handle_missing_values(df)\n\u001b[0;32m---> 28\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mremove_outliers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m df \u001b[38;5;241m=\u001b[39m scale_data(df)\n\u001b[1;32m     30\u001b[0m df \u001b[38;5;241m=\u001b[39m encode_categorical(df, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_column_name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mremove_outliers\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mremove_outliers\u001b[39m(df):\n\u001b[0;32m---> 12\u001b[0m     z_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[(z_scores \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mall(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/scipy/stats/_stats_py.py:2973\u001b[0m, in \u001b[0;36mzscore\u001b[0;34m(a, axis, ddof, nan_policy)\u001b[0m\n\u001b[1;32m   2891\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mzscore\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ddof\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, nan_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpropagate\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   2892\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2893\u001b[0m \u001b[38;5;124;03m    Compute the z score.\u001b[39;00m\n\u001b[1;32m   2894\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[38;5;124;03m           [-0.91611681, -0.89090508,  1.4983032 ,  0.88731639, -0.5785977 ]])\u001b[39;00m\n\u001b[1;32m   2972\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mzmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_policy\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/scipy/stats/_stats_py.py:3137\u001b[0m, in \u001b[0;36mzmap\u001b[0;34m(scores, compare, axis, ddof, nan_policy)\u001b[0m\n\u001b[1;32m   3135\u001b[0m         isconst \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(_isconst, axis, a)\n\u001b[1;32m   3136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3137\u001b[0m     mn \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3138\u001b[0m     std \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mstd(axis\u001b[38;5;241m=\u001b[39maxis, ddof\u001b[38;5;241m=\u001b[39mddof, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/numpy/core/_methods.py:182\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    180\u001b[0m ret \u001b[38;5;241m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 182\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrue_divide\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m            \u001b[49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munsafe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_float16_result \u001b[38;5;129;01mand\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m         ret \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(ret)\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'int'"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1755520922936
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.10 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}